<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Joy in Statitics</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://qswangstat.github.io/"/>
  <updated>2018-09-19T07:46:52.943Z</updated>
  <id>https://qswangstat.github.io/</id>
  
  <author>
    <name>Qingsong Wang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Handwritten Digits Recognition</title>
    <link href="https://qswangstat.github.io/2018/09/19/Handwritten-Digits-Recognition/"/>
    <id>https://qswangstat.github.io/2018/09/19/Handwritten-Digits-Recognition/</id>
    <published>2018-09-19T00:45:20.850Z</published>
    <updated>2018-09-19T07:46:52.943Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Abstract</strong> Statistical methods are widely used in image recognition or classification nowadays and can produce pretty satisfying results. MNIST database is a large public database of handwritten digits that is commonly used for training and testing recognizers or classifiers based on various statistical models. In this paper, several models- including discriminant analysis, k-nearest neighbor, kernel SVM, random forests, and neural networks- are described theoretically and then applied to this database. PCA helps to reduce data dimension in some cases. Testing error rates of all models are compared.<br><strong>Keywords:</strong> Statistical Learning, MNIST, Dimension Reduction, PCA, Discriminant Analysis, K-nearest Neighbor, Kernel SVM, Random Forests, Neural Network, Deep Learning, Testing Error Rate.<br><a id="more"></a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;MNIST is a handwritten digits image data set created by Yann LeCun. Each image is $28$ pixels in height and $28$ pixels in width i.e. $784$ pixels in total, so is represented as a vector in $784-D$ space. The training dataset includes $60000$ images, while the testing dataset $10000$ images.  </p><p>&nbsp;&nbsp;&nbsp;&nbsp;Our goal is to train different classifiers with training dataset and then use trained classifiers to recognize handwritten digits from testing dataset as accurately as possible.  </p><p>&nbsp;&nbsp;&nbsp;&nbsp; From the MNIST dataset, we can obtain large data matrix. The first column is the label of each image. The input pixels in the rest columns are greyscale value, with a value of $0.0$ representing white, a value of $1.0$ representing black, and values in $[0.0,1.0]$ representing gradually darkening shades of grey. To locate a specific pixel on the image, suppose that we have decomposed $x$ as $x = i * 28 + j$, where $i$ and $j$ are integers between $0$ and $27$, inclusive. Then pixel $x$ is located on row $i$ and column $j$ of a $28\times28$ matrix, (indexing by zero). Images can be restored as the following figure (1) shows.  </p><p><table style="border-style:none;" bgcolor="#FFFFFF">  <caption align="bottom"><center>Figure 1: Restored digit images</center></caption>    <tr bgcolor="#FFFFFF" style="border-style:none;">        <td bgcolor="#FFFFFF" style="border-style:none;"><center><img src="http://pfai2gb4l.bkt.clouddn.com/1.jpg" width="250"></center></td>        <td bgcolor="#FFFFFF" style="border-style:none;"><center><img src="http://pfai2gb4l.bkt.clouddn.com/1_2.jpg" width="250"></center></td>    </tr></table><br>&nbsp;&nbsp;&nbsp;&nbsp;In the field of image recognition and classification, statistical methods are widely used. Since the MNIST dataset is so large that some methods run very slow, we first introduce PCA, a common dimension reduction technique. Result show that the first $55 - 60$ principal components can explain over $85\%$ of total variance, so in some cases, we use those components instead of original data.<br>&nbsp;&nbsp;&nbsp;&nbsp;We then try different models and analyze the MNIST dataset from various perspectives. We apply k-nearest neighbor, SVM, random forests and neural network, roughly in the order of complexity. Given the sufficient training data and proper parameters, we all get pretty satisfying recognizers which perform well on testing data and the error rates can be controlled at a very low level. We give a curve of error rates in the last section.</p><h2 id="PCA-and-Dimension-Reduction"><a href="#PCA-and-Dimension-Reduction" class="headerlink" title="PCA and Dimension Reduction"></a>PCA and Dimension Reduction</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;Principal component analysis(PCA) is an nonparametric method, which converts a set of observations of correlated variables into a set of values of linearly uncorrelated variables, i.e. principal components.<br>&nbsp;&nbsp;&nbsp;&nbsp;For a random vector $\mathbf{x}\in \mathbb{R}^p$ with covariance structure $cov(\mathbf{x}) = \sigma$, then the principal components of population are $\alpha_i\mathbf{x}, i = 1,2,\cdots,p$.<br>&nbsp;&nbsp;&nbsp;&nbsp;For data sample, suppose the data matrix $\mathbf{X}_{n\times p} = (X_1,\cdots, X_p)$, where $n$ is the number of observations. Assume $rank(\mathbf{X}) = p &lt; n$, and $\mathbf{X}$ has been centralized i.e. $\mathbf{1}^TX_i = 0, i = 1,2,\cdots,p$. Then $\text{cov}(\mathbf{X}) = \frac{1}{n-1}\mathbf{X}^T\mathbf{X}$.<br>&nbsp;&nbsp;&nbsp;&nbsp;Suppose $\mathbf{X}$ has singular value decomposition: $\mathbf{X} = \mathbf{UDV}^T$, then $\mathbf{X}^T\mathbf{X} = \mathbf{VD}^2\mathbf{V}^T$, which means the diagonal elements of $\mathbf{D}$ are eigenvalues of $\mathbf{X}^T\mathbf{X}$ and the columns of $\mathbf{V}$ are corresponding eigenvectors. $\mathbf{XV}$ denotes the PC score.<br>&nbsp;&nbsp;&nbsp;&nbsp;For the training data $\mathbf{X}^{train}$ from MNIST, $n = 60000, p = 784$, the variances of the first $60$ components and cumulative variance proportion are shown below in figure (2) and (3).<br>&nbsp;&nbsp;&nbsp;&nbsp;We find that the first $60$ components explain over $85\%$ of total variance. Denote $\mathbf{T}$ as the first $60$ columns of $\mathbf{V}$, it is acceptable to use $\mathbf{X}^{train}\mathbf{T}, \mathbf{X}^{test}\mathbf{T}$ instead of original data. By applying PCA, we reduce the dimension MNIST data, which helps to speed up some algorithms. Next we visualize the results of PCA.</p><p><table style="border-style:none;" bgcolor="#FFFFFF">    <tr bgcolor="#FFFFFF" style="border-style:none;">        <td bgcolor="#FFFFFF" style="border-style:none;"><center><img src="http://pfai2gb4l.bkt.clouddn.com/2.jpg" width="250">Figure 2: Variance of ﬁrst $60$ PCs </center></td>        <td bgcolor="#FFFFFF" style="border-style:none;"><center><img src="http://pfai2gb4l.bkt.clouddn.com/3.jpg" width="250">Figure 3: Cumulative variance proportion of PCs </center></td>    </tr></table><br>&nbsp;&nbsp;&nbsp;&nbsp;Figure (4) shows the “eigenfaces” from the first $18$ PC loadings. Then we reconstruct the images based on the first $60$ PCs.</p><p><table style="border-style:none;" bgcolor="#FFFFFF"></table></p><p><caption align="bottom"><center>Figure 4: First $18$ eigenfaces</center></caption><br>    <tr bgcolor="#FFFFFF" style="border-style:none;">        <td bgcolor="#FFFFFF" style="border-style:none;"><center><img src="http://pfai2gb4l.bkt.clouddn.com/4.jpg" width="250"></center></td>        <td bgcolor="#FFFFFF" style="border-style:none;"><center><img src="http://pfai2gb4l.bkt.clouddn.com/4_2.jpg" width="250"></center></td>    </tr><br>&lt;/table&gt;<br>&nbsp;&nbsp;&nbsp;&nbsp;Figure (5) shows comparison between raw images and reconstructed ones. It proves again that using the first $60$ PCS is an acceptable choice.  </p><p><table style="border-style:none;" bgcolor="#FFFFFF"></table></p><p><caption align="bottom"><center>Figure 5: Comparison: raw VS reconstructed</center></caption><br>    <tr bgcolor="#FFFFFF" style="border-style:none;">        <td bgcolor="#FFFFFF" style="border-style:none;"><center><img src="http://pfai2gb4l.bkt.clouddn.com/5.jpg" width="250"></center></td>        <td bgcolor="#FFFFFF" style="border-style:none;"><center><img src="http://pfai2gb4l.bkt.clouddn.com/5_2.jpg" width="250"></center></td>    </tr><br>    <caption>Figure 1: Restored digit images</caption><br>&lt;/table&gt;</p><h2 id="Discriminant-Analysis"><a href="#Discriminant-Analysis" class="headerlink" title="Discriminant Analysis"></a>Discriminant Analysis</h2><p>Consider modelling $P(Y = k|X = x), k = 1,2,\cdots,10$, discriminant analysis model the distribution of the predictors $X$ given $Y$ and then estimate $P(Y = k|X = x)$ by Bayesian theorem i.e.</p><script type="math/tex; mode=display">    \begin{aligned}    P(Y=k|X=x) = \frac{\pi_kP(X=x|Y=k)}{\sum_{l=1}^K\pi_lP(X=x|Y=l)} := \frac{\pi_kf_k(x)}{\sum_{l=1}^K\pi_lf_l(x)},\qquad(1)    \end{aligned}</script><p>where $\pi_k$ denotes the prior probability of the $k$th class.</p><h3 id="Linear-Discriminant-Analysis"><a href="#Linear-Discriminant-Analysis" class="headerlink" title="Linear Discriminant Analysis"></a>Linear Discriminant Analysis</h3><p>LDA recognizer assumes that the observation from the $k$th digit are drawn from Gaussian distribution $N(\mu_k, \Sigma)$, where $\Sigma$ is common among all digits. Then</p><script type="math/tex; mode=display">    f_k(x) = C\exp\left[-\frac{1}{2}(x-\mu_k)^T\Sigma^{-1}(x-\mu_k)\right],\qquad (2)</script><p>where $C$ is a constant. Plugging equation (2) into (1) reveals that LDA assigns $X = x$ to the class where</p><script type="math/tex; mode=display">    \delta_k(x) = x^T\Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma^{-1}\mu_k + \log\pi_k</script><p>is largest.<br>Several pixels of original MNIST dataset are constant which LDA cannot handle, so we use the first $60$ principal components instead and do related analysis. LDA generates a rough linear classifier, whose error rate on testing data is $12.38\%$. This means that our assumption that all digits share the same covariance structure is not very reasonable. So we turn to QDA.</p><h3 id="Quadratic-Discriminant-Analysis"><a href="#Quadratic-Discriminant-Analysis" class="headerlink" title="Quadratic Discriminant Analysis"></a>Quadratic Discriminant Analysis</h3><p>QDA assumes the $k$th class has own covariance matrix $\Sigma_k$. Under this assumption, QDA recognizer assigns an observation $X = x$ to the class where</p><script type="math/tex; mode=display">    \delta_k(x) = -\frac{1}{2}x^T\Sigma_k^{-1}x + x^T\Sigma_k^{-1}\mu_k - \frac{1}{2}\mu_k^T\Sigma_k^{-1}\mu_k - \frac{1}{2}\log\det{\Sigma_k} + \log\pi_k</script><p>is largest.<br>&nbsp;&nbsp;&nbsp;&nbsp;Again for the same reason, we use the first $60$ PCs instead. This time the error rate decreases to $3.88\%$, which improves a lot compared to LDA. This result shows the covariance structure is indeed different among digits, which we guess is probably due to different structures of digits.  We are not very satisfied with the performance of discriminant analysis, so we consider a nonparametric model— k-nearest neighbor.</p><h2 id="K-nearest-Neighbor"><a href="#K-nearest-Neighbor" class="headerlink" title="K-nearest Neighbor"></a>K-nearest Neighbor</h2><p>Given $x_0$ to be recognized, KNN recognizer picks $K$ points in the training data that are closest to $x_0$, denoted as $N_0$. Then it estimates</p><script type="math/tex; mode=display">    P(Y = k|X= x_0) = \frac{1}{K}\sum_{i\in N_0}I_{(y_i = k)}.</script><p>Finally, it classifies $x_0$ to the class with the largest probability. KNN is quite intuitive but lacks in efficiency and the choice of $K$ is tricky.<br>&nbsp;&nbsp;&nbsp;&nbsp;We apply KNN to original data and PCs respectively and set $K = 5$. The error rate is $3.12\%$ for the former and $2.47\%$ for the latter. PCs may filter uncorrelated pixels that even worsen the performance of KNN. Next we will try some more complex models in statistical learning.</p><h2 id="Kernel-Support-Vector-Machine"><a href="#Kernel-Support-Vector-Machine" class="headerlink" title="Kernel Support Vector Machine"></a>Kernel Support Vector Machine</h2><p>For a two-class recognition problem with non-linear class boundaries, kernel SVM enlarges the feature space to accommodate the boundaries, by using various kernels including polynomial kernel $K(x_i,x_j) = (1+\langle x_i,x_j\rangle)^d$, Gaussian kernel $K(x_i,x_j) = \exp(-\gamma|x_i-x_j|^2)$, etc. First project the data into a high-dimensional space</p><script type="math/tex; mode=display">    x_i :\rightarrow\phi(x_i,\cdot),</script><p>Then define the inner product $K(x_i,x_j)$, where $K(\cdot,\cdot)$ is a non-linear kernel function. The recognizer $f(x)$ has the form</p><script type="math/tex; mode=display">    f(x) = \beta_0 + \sum_{i\in S}\alpha_iy_iK(x,x_i).\qquad (3)</script><p>&nbsp;&nbsp;&nbsp;&nbsp;The coefficients in equation (3) can be obtained by solving  a constrained optimization problem, where the dual function is given by</p><script type="math/tex; mode=display">L_d = \sum_i\alpha_i - \frac{1}{2}\sum_i\sum_j\alpha_i\alpha_jy_iy_jK(x_i,x_j).</script><p>&nbsp;&nbsp;&nbsp;&nbsp;If the recognition problem extends to $K$-class case, there are two popular ways: <em>one-versus-one</em> and <em>one-versus-all</em>. The former one trains $\frac{K(K-1)}{2}$ SVMs, each of which recognizes a pair of classes. The latter one trains $(K-1)$ SVMs, each of which recognizes one class from all other classes.<br>&nbsp;&nbsp;&nbsp;&nbsp;We try three kinds of SVM and their performances vary a bit, as listed below.</p><ul><li>SVM: Gaussian kernel, $2.68\%$, trained with original data;</li><li>SVM: Gaussian kernel, $2.58\%$, trained with the first $60$ PCs;</li><li>SVM: $3$-order Polynomial kernel, $2.01\%$, trained with the first $60$ PCs.</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;It seems the last one outperforms the other two and again using PCs is a better choice. Limited by time and space, we don’t tune the order of polynomials or the kind of kernel functions, but these are worth trying.</p><h2 id="Random-Forests"><a href="#Random-Forests" class="headerlink" title="Random Forests"></a>Random Forests</h2><p>Random forests build a large collection of de-correlated trees and then average them. The implementation is described as follows.</p><blockquote><ul><li>For $l = 1,2,\cdots,L$<ul><li>Draw a bootstrap sample;<ul><li>Grow a tree $T_l$ to the sample:<ul><li>Select $m$ variables, $m \leq p$;</li><li>Pick the best split point and split the node into two;</li></ul></li></ul></li></ul></li><li>Ensemble trees $\{T_l\}$;</li><li>The classification result $C_{rf}(x) = \arg\max #\{k: C_l(x) = k\}$.</li></ul></blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;Random forests often perform very well in image recognition. But when we set $L = 25$, the error rate reaches $3.83\%$, which is an ordinary result. The reason may be the number of trees needs to be larger, but larger number means more cost of time and RAM, so we has to make the trade-off.</p><h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><p>First we review some important definitions in neural network theory. A neuron is a real-value function defined on $\mathbb{R}^d$ given by</p><script type="math/tex; mode=display">    g(x) = \sigma(a^Tx + b),</script><p>where $\sigma(\cdot)$ is also called an activation function. A hidden layer contains $k$ neurons and corresponds to a non-linear transformation:</p><script type="math/tex; mode=display">    f(x) = \sum_{i=1}^kc_i\sigma(a_i^Tx + b_i) + c_0.</script><p>&nbsp;&nbsp;&nbsp;&nbsp;Traditional neural networks are trained by back propagation method, which is actually an iterative method. This will lead to poor efficiency and overfitting problem. Nowadays deep learning methods try to train layer-wise.</p><h3 id="Fully-Connected-Neural-Network"><a href="#Fully-Connected-Neural-Network" class="headerlink" title="Fully Connected Neural Network"></a>Fully Connected Neural Network</h3><p>In this case, we try a four-layer neural network:</p><ul><li>Input layer: neurons encoding the values of input pixels;</li><li>First hidden layer: $128$ neurons, activation function is $f(x) = \max(x,0)$;</li><li>Second hidden layer: $64$ neurons, activation function is $f(x) = \max(x,0)$;</li><li>Output layer: $10$ neurons corresponding to $10$ digits, and softmax is used to get a probabilistic prediction.</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp;We call R package “mxnet” to train our network and get a error rate of $2.05\%$. Also we try to replace original data with PCs and the error rate increases slightly to $2.40\%$.</p><h3 id="Convolutional-Neural-Network"><a href="#Convolutional-Neural-Network" class="headerlink" title="Convolutional Neural Network"></a>Convolutional Neural Network</h3><p>Convolutional neural network has three key points— local receptive fields, shared weights and pooling. The idea is that a small kernel window is moved over neurons from a prior layer. Every neuron is given by</p><script type="math/tex; mode=display">    \sigma(b + \text{local receptive field}\otimes w),</script><p>where $\otimes$ means convolution. Each pair of $b, w$ correspond to a feature map and there can be several different maps on a single layer. Note that weights are shared so the number of parameters is much smaller than fully connected network. Then based on these maps, pooling layers are built, often by max pooling.<br>&nbsp;&nbsp;&nbsp;&nbsp;In this case, the structure of CNN is summarized below.</p><ul><li>First convolution: $5\times 5$ kernel, $20$ filters, activation function is $\tanh$, max pooling with $2\times 2$ kernel;</li><li>Second convolution: $5\times 5$ kernel, $50$ filters, activation function is $\tanh$, max pooling with $2\times 2$ kernel;</li><li>First full connection: $500$ neurons, activation function is $\tanh$;</li><li>Second full connection: $10$ neurons;</li><li>Output: sofmax is used to get a probabilistic prediction.</li></ul><p>&nbsp;&nbsp;&nbsp;&nbsp; Note that we need to reshape our original data first. And CNN finally reaches an error rate of $1.00\%$, which is very impressive.</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Figure (6) summarizes the performances of all recognizers. In fact, every result is pretty satisfying except LDA. Thanks to the large amount of training data, every recognizer is trained sufficiently. But some recognizers which are expected to produce surprising results, such as random forests, performs not that well. We think the reason is that some parameters are chosen empirically thus not very proper, weakening the overall performance. There are many tricky parameters that are listed in table (1). So how to choose “right” parameters is another topic to be studied in the future.</p><center class="center">    <img src="http://pfai2gb4l.bkt.clouddn.com/6.jpg" width="400"> Figure 6: Testing error rates of all recognizers</center><div class="table-container"><table><thead><tr><th>Models</th><th>PCA</th><th>LDA &amp; QDA</th><th>KNN</th></tr></thead><tbody><tr><td>Parameters</td><td>Number   of PCs</td><td>None</td><td>Number   of neighbors</td></tr><tr><td>Models</td><td>SVM</td><td>Random Forests</td><td>Neural Network</td></tr><tr><td>Parameters</td><td>Kernel function</td><td>Number   of trees</td><td>Number of neurons,  Number of layers,  Kernel size,  Number of feature maps,  etc.</td></tr></tbody></table></div><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Hastie T, Tibshirani R, Friedman J. The Elements of Statistical Learning[M]// The elements of statistical learning :. Springer, 2003:192-192.<br>[2] James G, Witten D, Hastie T, et al. An Introduction to Statistical Learning[M]. Springer New York, 2013.<br>[3] Lecun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt; Statistical methods are widely used in image recognition or classification nowadays and can produce pretty satisfying results. MNIST database is a large public database of handwritten digits that is commonly used for training and testing recognizers or classifiers based on various statistical models. In this paper, several models- including discriminant analysis, k-nearest neighbor, kernel SVM, random forests, and neural networks- are described theoretically and then applied to this database. PCA helps to reduce data dimension in some cases. Testing error rates of all models are compared.&lt;br&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; Statistical Learning, MNIST, Dimension Reduction, PCA, Discriminant Analysis, K-nearest Neighbor, Kernel SVM, Random Forests, Neural Network, Deep Learning, Testing Error Rate.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="project" scheme="https://qswangstat.github.io/categories/project/"/>
    
    
      <category term="machine learning" scheme="https://qswangstat.github.io/tags/machine-learning/"/>
    
  </entry>
  
</feed>
